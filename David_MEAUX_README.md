# David's Approach
J'ai fini par utiliser une machine Windows que j'avais donnée à mon fils, j'y ai réinstallé MySQL et j'ai redémarré le projet depuis le début le mardi. J'ai fait cela parce qu'en utilisant phpMyAdmin avec MySQL sur la machine virtuelle, j'ai rencontré des problèmes avec l'importation des deux plus grandes tables (les commandes et les détails de la ligne de commande). Chaque fois que j'ai exécuté la séquence d'importation des scripts SQL sur la machine virtuelle en utilisant phpMyAdmin, le nombre total d'enregistrements importés pour ces deux tables ne correspondait pas au nombre d'enregistrements à importer, de sorte que je n'avais pas confiance dans le système et que je ne pouvais donc pas vérifier que mes requêtes s'exécutaient comme je l'avais prévu.

Une fois que les données ont été stockées dans une base de données sur un serveur MySQL local, mon approche a consisté à créer un instantané de tous les articles de commande qui avaient chacun une commande et un client associés (certains n'avaient pas de commande ou de client associé). Le script SQL que j'ai créé crée une jointure interne entre les tables commandes (T_entcde) et les détails de la commande (T_dtlcde), suivie d'une jointure gauche avec la table objets (T_objet), et enfin une autre jointure interne avec la table clients (T_client). Il enregistre ensuite les résultats dans une table d'instantanés où l'horodatage actuel est utilisé pour créer le nom de la table. Cela permettra de suivre la date à laquelle chaque instantané de commande a été réalisé à des fins de lignage des données.

Ce tableau plat a ensuite été importé dans Power BI. Au cours de l'étape de transformation des données, j'ai créé une requête Group By qui a regroupé les détails de la commande dans leur commande associée et a calculé le nombre total d'articles et le nombre moyen d'articles pour chaque commande. En outre, j'ai ajouté une nouvelle colonne Département qui a été créée à partir des deux premiers chiffres de la chaîne du code postal.

Ensuite, j'ai utilisé des requêtes DAX pour créer une colonne de rang qui ajoutait le nombre d'articles pour chaque commande à la valeur du timbre de la commande. J'ai également créé des requêtes DAX pour créer de nouvelles tables pour chaque requête importante à effectuer. J'ai adopté cette approche parce qu'elle me permettait de vérifier les résultats de mes requêtes. Après mon expérience avec la VM, j'étais très inquiet que mes requêtes fassent ce que je voulais qu'elles fassent, et cela m'a permis d'atténuer ces inquiétudes. Si le client avait eu besoin de filtres dynamiques pour les résultats, j'aurais pu utiliser ces requêtes pour vérifier que j'avais utilisé les bons paramètres de filtrage pour chaque visualisation.

Il convient de noter que la table Départements dans les requêtes d'exportation SQL des clients ne contenait pas les informations nécessaires pour lier le nom du département à tous les enregistrements, en raison de l'absence de séparation claire entre la Corse du Nord et la Corse du Sud. Par conséquent, un fichier de données contenant tous les codes postaux, villes, départements et régions de France a été téléchargé depuis l'INSEE, juste au cas où le client souhaiterait que les noms de départements soient correctement ajoutés aux visualisations de données.

---
### English, just in case something got lost in translation.

I ended up using a Windows machine I had given to my son, reinstalling MySQL on it, and restarting the project from scratch on Tuesday. I did this because using phpMyAdmin with MySQL on the virtual machine I ran into issues with importing the two largest tables (the order records and the order line details). Each time I ran the import sequence of SQL scripts on the virtual machine using phpMyAdmin the number of total records that were imported for these two tables did not match the number of records to be imported, so I did not have confidence in the system and could, therefore, not verify that my queries were executing the way I intended.

Once, I had the data in a database on a local MySQL server, my approach was to create a snapshot of all of the order line items that each had an associated order and an associated client (some were missing a related order or client). The SQL script I created creates an inner join between the orders (T_entcde) and the order details (T_dtlcde) tables, followed by a left join with the objects table (T_objet), and finally another inner join with the clients (T_client) table. It then saves the results into a snapshot table where the current timestamp is used to create the table name. This will provide a means to track when each order snapshot was made for data lineage purposes.

This flat table was then imported into Power BI. During the data transform stage, I created a Group By query that grouped the order details into their associated order and calculated the total number of articles and average number of articles for each order. In addition, I added a new Department column that was created from the first two digits of the postal code string.

Afterwards, I used DAX queries to create a rank column that added the number of items for each order to the order's stamp value. I also created DAX queries to create new tables for each major query that needed to be completed. I took this approach because it allowed me to verify my query results. After my experience with the VM, I was very worried about my queries doing what I wanted them to do, and this allowed my to alleviate those worries. If the client had needed dynamic filters for the results, I could have used these queries to verify that I had used the correct filter parameters for each visualization.

Of note, the Departments table in the Clients SQL export queries did not contain the required information for linking the department name with all of the records, due to there not being a clear separation between North and South Corsica. Therefore, a data file containing all of France's postal codes, towns, departments, and regions was downloaded from INSEE, just in case the client wanted the Department names correctly added to the data visualizations.